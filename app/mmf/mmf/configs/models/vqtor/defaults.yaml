model_config:
  vqtor:
    # Type of bert model
    #bert_model_name: bert-base-uncased
    direct_features_input: false
    # Dimension of the embedding finally returned by the modal encoder
    modal_hidden_size: 2048
    # Dimension of the embedding finally returned by the text encoder
    text_hidden_size: 768
    # Used when classification head is activated
    #num_labels: 10 # TODO: ?
    # Number of features extracted out per image
    num_features: 1

    img_dim: 2048
    ques_dim: 768
    kg_dim: false


    # resnet50
    image_encoder:
      type: torchvision_resnet
      params:
        name: resnet50
        pretrained: true
        pool_type: avg
        num_output_features: 1
        zero_init_residual: false

    text_encoder:
      type: any_transformer
      params:
        name: distilbert-base-uncased
        dim: 768
        num_hidden_layers: 12
        num_attention_heads: 12

    graph_module:
      dataset_info_path: okvqa/defaults/annotations/annotations/graph_vocab/okvqa_dataset_info.pth.tar
      # dimension difference 2553 and 2550
      vocab_file: okvqa/defaults/annotations/annotations/answer_vocab_count10.txt
      kg_path: okvqa/defaults/annotations/annotations/graphs/cn_graph.pth.tar
      node2vec_filename: okvqa/defaults/annotations/annotations/node2vec/node2vec_cn.pkl
      graph_vocab_file: okvqa/defaults/annotations/annotations/graph_vocab/graph_vocab_cn.pth.tar
      prune_culdesacs: false
      use_w2v: true
      # what type of inputs
      node_inputs:
        # numbers are not used in module
        question: 1
        #classifiers: 4 # 4 probabilities whether the node/word/concept is in the image
        w2v: 300 # GloVe representation of the concept
      output_type: graph_prediction # [output_type]
      output_order: alpha # alphabetic, same as answer vocab
      output_special_node: false
      gcn_type: RGCN # Relational Graph Convolution Network
      num_gcn_conv: 2
      use_batch_norm: true
      use_dropout: false
      dropout_p: 0
      add_ans_nodes: false
      feed_special_node: false
      num_labels: 2250
      # since this will be the input
      node_hid_dim: ${dataset_config.okvqa.processors.text_processor.params.max_seq_length}  #128 # TODO: same as tokenization?
      okvqa_v_mode: v1.1
      analysis_mode: false
      # TODO: useless?
      noback_vb_to_graph: false
      use_conf: false # ?
      graph_logit_mode: in_graph # [in_graph, logit_fc]
      output_combine: concat # [add, concat]
      feed_q_to_graph: false # not to be inputted, only if concept is or isn't in q
      num_labels: 2250 # if inputted in fc





    classifier:
      type: mlp
      params:
        # 2048 + 768 + 1310 in case of features
        in_dim: 4126 # TODO: more auto?
        out_dim: 2250 # target size for okvqa (okvqa v1.1 has 2250) todo: autoindex_in_node
        hidden_dim: 3188 # mean of input and output
        num_layers: 2 # ?